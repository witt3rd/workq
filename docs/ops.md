---
status: active
milestone: kernel
spec: null
code: docker-compose.yml, docker/, src/telemetry/
---

# Operations

*Observability, backups, alerting, and operational runbook for the animus appliance.*

## Stack

Every animus instance runs a full observability stack alongside the agent service:

```
animus-rs (daemon or CLI)
    → OTLP gRPC (:4317)
        → OTel Collector
            → Tempo (traces)
            → Prometheus (metrics, via remote write)
            → Loki (logs, via OTLP)
    → Grafana (:3000) — unified UI
```

| Service | Image | Internal Port | Host Port (dev) | Data Path |
|---|---|---|---|---|
| Postgres | custom (pgmq + pgvector) | 5432 | `ANIMUS_PG_PORT` (5432) | `{data}/postgres/` |
| OTel Collector | `otel/opentelemetry-collector-contrib` | 4317, 4318 | `ANIMUS_OTEL_GRPC_PORT` (4317) | stateless |
| Tempo | `grafana/tempo:2.7.2` | 3200 | none | `{data}/tempo/` |
| Loki | `grafana/loki:latest` | 3100 | none | `{data}/loki/` |
| Prometheus | `prom/prometheus:latest` | 9090 | none | `{data}/prometheus/` |
| Grafana | `grafana/grafana:latest` | 3000 | `ANIMUS_GRAFANA_PORT` (3000) | `{data}/grafana/` |

Where `{data}` = `${ANIMUS_HOME}/data/${ANIMUS_INSTANCE}/` (default: `~/.animus/data/dev/`).

## Data Durability

All persistent state lives on host-mounted volumes — not Docker-managed volumes. This means:

- **Time Machine / filesystem backups cover everything** — Postgres data, Grafana config, metrics, traces, logs
- **`docker compose down` does not destroy data** — containers are ephemeral, data is on the host
- **`docker compose down -v` is safe** — there are no Docker volumes to remove
- **Multiple instances are isolated** — each instance has its own directory under `{ANIMUS_HOME}/data/`

### Directory Layout

```
~/.animus/                          # ANIMUS_HOME
  data/
    dev/                            # ANIMUS_INSTANCE
      postgres/                     # Postgres PGDATA (WAL, tables, indexes)
      tempo/                        # Trace storage
      loki/                         # Log storage (chunks, WAL, index)
      prometheus/                   # Metric storage (TSDB)
      grafana/                      # Grafana SQLite (dashboards, alerts, users)
    kelly/                          # another instance
      ...
```

## Backups

### Postgres (Critical — Domain State)

Postgres holds the source of truth: work items, queue messages, memories, the work ledger (future). This is the one thing that MUST be backed up.

**Ad-hoc backup:**
```sh
docker compose exec postgres pg_dump -U animus animus_dev > backup_$(date +%Y%m%d_%H%M%S).sql
```

**Automated daily backup (cron / launchd / systemd timer):**
```sh
#!/bin/bash
BACKUP_DIR="${ANIMUS_HOME:-$HOME/.animus}/backups/${ANIMUS_INSTANCE:-dev}"
mkdir -p "$BACKUP_DIR"
docker compose exec -T postgres pg_dump -U animus animus_dev \
  > "$BACKUP_DIR/animus_$(date +%Y%m%d_%H%M%S).sql"

# 30-day retention
find "$BACKUP_DIR" -name "animus_*.sql" -mtime +30 -delete
```

**Restore:**
```sh
docker compose exec -T postgres psql -U animus animus_dev < backup_file.sql
```

### Observability Data (Important — Not Critical)

Tempo, Loki, Prometheus data is valuable for debugging and auditing but can be regenerated by re-running the agent. If lost, you lose historical telemetry but not domain state.

These are covered automatically if `~/.animus/data/` is on a backed-up filesystem (Time Machine, rsync, etc.).

**Prometheus retention:** configured at 30 days (`--storage.tsdb.retention.time=30d`).
**Loki retention:** configured at 30 days (`limits_config.retention_period: 30d`).
**Tempo retention:** not explicitly configured — defaults vary by version.

### Grafana State (Convenient — Recreatable)

Grafana's SQLite holds dashboards, alert rules, contact points, and user preferences. If lost, dashboards and alerts need to be recreated. Datasources are provisioned from YAML and auto-restore.

Covered by filesystem backups of `{data}/grafana/`.

## Three-Signal Telemetry

### Traces (Tempo)

Every work item execution creates a trace:

```
work.execute
  ├── work.orient
  ├── work.engage
  │     ├── work.engage.iteration[1]
  │     │     ├── gen_ai.chat
  │     │     └── work.tool.execute[...]
  │     └── ...
  ├── work.consolidate
  └── work.recover (if needed)
```

**View in Grafana:** Explore → Tempo → Search by `service.name = animus`

Trace attributes follow GenAI semantic conventions:
- `gen_ai.operation.name`, `gen_ai.request.model`, `gen_ai.provider.name`
- `gen_ai.usage.input_tokens`, `gen_ai.usage.output_tokens`

### Metrics (Prometheus)

All metrics are prefixed with `animus_`:

| Metric | Type | Labels | Description |
|---|---|---|---|
| `animus_work_submitted_total` | Counter | faculty, result | Work items submitted |
| `animus_work_state_transitions_total` | Counter | from, to | State transitions |
| `animus_work_unroutable_total` | Counter | faculty | Work with no matching faculty |
| `animus_queue_operations_total` | Counter | queue, operation | pgmq operations |
| `animus_memory_operations_total` | Counter | operation | Memory store operations |
| `animus_llm_tokens_total` | Counter | model, provider, direction | LLM token usage |
| `animus_operation_duration_ms_milliseconds` | Histogram | operation | Operation duration |

**View in Grafana:** Explore → Prometheus → query metric name

### Logs (Loki)

All `tracing::info!`, `warn!`, `error!` calls are exported to Loki via the OTel log bridge. Structured fields (faculty, work_id, etc.) are preserved as log labels.

**View in Grafana:** Explore → Loki → `{service_name="animus"}`

Filter by level: `{service_name="animus"} |= "WARN"`

Logs include trace IDs — click through from a log line to the corresponding trace in Tempo.

## Alerting

Alert rules are managed in Grafana (Alerting → Alert rules). They persist in Grafana's SQLite at `{data}/grafana/grafana.db`.

### Current Alert Rules

| Alert | Query | Condition | Severity |
|---|---|---|---|
| Work with no faculty | `sum(animus_work_unroutable_total)` | > 0 | warning |

### Creating Alert Rules

Via Grafana UI: Alerting → Alert rules → New alert rule.
Via API:
```sh
curl -X POST http://localhost:3000/api/v1/provisioning/alert-rules \
  -H "Content-Type: application/json" \
  -d '{ ... }'
```

### Contact Points (Future)

Alerts currently show in the Grafana UI only. To receive notifications:

1. **Grafana → Alerting → Contact points → Add contact point**
2. Choose integration: Slack, PagerDuty, Email, Discord, webhook, etc.
3. **Grafana → Alerting → Notification policies**
4. Route by label: `severity=warning` → Slack, `severity=critical` → PagerDuty

Contact point configuration persists in Grafana's SQLite — no code changes needed.

### Planned Alert Rules

| Alert | When to Add | Query | Severity |
|---|---|---|---|
| Focus failure rate | M5a (engage loop) | `rate(animus_work_state_transitions_total{to="failed"}[5m]) > 0.5` | critical |
| High LLM token usage | M3 (LLM client) | `rate(animus_llm_tokens_total[1h]) > threshold` | warning |
| Queue depth growing | Now (already possible) | `delta(animus_queue_operations_total{operation="send"}[1h]) - delta(animus_queue_operations_total{operation="archive"}[1h]) > 10` | warning |
| Emergency summarizations | M5c (compaction) | `animus_work_context_emergency_summarizations_total > 0` | warning |

## Operational Commands

### Start / Stop

```sh
docker compose up -d                   # start all services
docker compose down                    # stop all services (data preserved)
docker compose restart grafana         # restart a single service
docker compose logs -f otel-collector  # follow logs for a service
```

### Daemon

```sh
cargo run --bin animus -- serve                  # run control plane
cargo run --bin animus -- serve --faculties DIR   # custom faculty dir
```

### Work Management (CLI)

```sh
animus work submit implement bootstrap --dedup-key "milestone=M4" --params '{...}'
animus work list
animus work list --state queued
animus work show b554bcb3
```

### Database

```sh
# Connect to Postgres
docker compose exec postgres psql -U animus animus_dev

# Check tables
docker compose exec postgres psql -U animus animus_dev -c "\dt"

# Count work items by state
docker compose exec postgres psql -U animus animus_dev -c "
  SELECT state, count(*) FROM work_items GROUP BY state ORDER BY count DESC;
"

# Run migrations
cargo run --bin animus -- serve  # migrations run on startup
# or manually:
# cargo sqlx migrate run
```

### Health Checks

```sh
# Postgres
docker compose exec postgres pg_isready -U animus

# Grafana
curl -s http://localhost:3000/api/health | python3 -m json.tool

# OTel Collector (check it's listening)
curl -s http://localhost:4318/v1/traces -X POST -d '{}' 2>&1 | head -1

# Prometheus (check targets)
# In Grafana: Explore → Prometheus → up
```

## Multi-Instance

Multiple animus instances can run on the same machine:

```sh
# Instance 1 (default)
ANIMUS_INSTANCE=dev docker compose up -d

# Instance 2 (different ports, different data)
ANIMUS_INSTANCE=kelly ANIMUS_PG_PORT=5433 ANIMUS_GRAFANA_PORT=3001 \
  COMPOSE_PROJECT_NAME=animus-kelly docker compose up -d
```

Each instance gets:
- Its own Docker network (via `COMPOSE_PROJECT_NAME`)
- Its own data directory (`~/.animus/data/{instance}/`)
- Its own port mappings (no conflicts)

## Configuration Reference

| Env Var | Default | Description |
|---|---|---|
| `ANIMUS_HOME` | `~/.animus` | Data root directory |
| `ANIMUS_INSTANCE` | `dev` | Instance name |
| `COMPOSE_PROJECT_NAME` | `animus-dev` | Docker project name |
| `DATABASE_URL` | `postgres://animus:animus_dev@localhost:5432/animus_dev` | Postgres connection (host) |
| `OTEL_ENDPOINT` | `http://localhost:4317` | OTLP gRPC endpoint |
| `LOG_LEVEL` | `info` | Tracing filter level |
| `ANTHROPIC_API_KEY` | — | LLM API key (secret) |
| `ANIMUS_PG_PORT` | `5432` | Host port for Postgres |
| `ANIMUS_GRAFANA_PORT` | `3000` | Host port for Grafana |
| `ANIMUS_OTEL_GRPC_PORT` | `4317` | Host port for OTel gRPC |
| `ANIMUS_OTEL_HTTP_PORT` | `4318` | Host port for OTel HTTP |
